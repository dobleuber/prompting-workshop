{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is **a vast expanse of atmosphere above the Earth, often appearing blue during the day.**\n",
      "\n",
      "Of course, depending on the time of day, weather conditions, and location, the sky can be many other things:\n",
      "\n",
      "*   **Blue:** During the day, due to Rayleigh scattering of sunlight.\n",
      "*   **Gray:** When it's cloudy.\n",
      "*   **Orange, Pink, or Red:** At sunrise and sunset.\n",
      "*   **Black:** At night.\n",
      "*   **Full of stars:** At night, away from light pollution.\n",
      "\n",
      "What did you have in mind when you asked?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "print(model.generate_content(\"The sky is\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seamos un poco mas especificos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is **blue**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Complete the sentence: \n",
    "The sky is \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formateo de Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting effective prompts for large language models (LLMs) to elicit desired responses. It involves carefully designing and refining the input text given to an LLM to guide its output and achieve a specific goal. Think of it as speaking the language the AI understands to get the best results.\n",
      "\n",
      "Here's a breakdown of what that means:\n",
      "\n",
      "*   **Crafting:** Prompt engineering isn't just about typing a question. It's about structuring the input strategically. This can involve using specific wording, formatting, keywords, and providing context.\n",
      "*   **Effective Prompts:** The goal is to create prompts that are clear, concise, and unambiguous so the LLM understands what is being asked and how to respond.\n",
      "*   **Large Language Models (LLMs):** LLMs are AI models trained on vast amounts of text data. They can generate text, translate languages, answer questions, and more. Examples include GPT-3, LaMDA, and others.\n",
      "*   **Desired Responses:**  The whole point of prompt engineering is to get the LLM to produce the specific type of output you need. This could be a creative story, a summary of a document, code generation, answering a question with a specific format, or many other tasks.\n",
      "\n",
      "**Why is it important?**\n",
      "\n",
      "*   **Improved Accuracy and Relevance:** Well-engineered prompts can significantly improve the accuracy, coherence, and relevance of LLM responses. Poor prompts can lead to vague, nonsensical, or even incorrect outputs.\n",
      "*   **Control and Specificity:** Prompt engineering allows you to exert more control over the LLM's output.  You can steer the model towards a particular style, tone, or perspective.\n",
      "*   **Cost Optimization:**  Better prompts often require fewer attempts to get the desired outcome, saving on API costs and computational resources.\n",
      "*   **Unlocking Potential:** It unlocks the full potential of LLMs by enabling them to perform more complex and nuanced tasks.\n",
      "\n",
      "**Key Aspects of Prompt Engineering:**\n",
      "\n",
      "*   **Clarity and Specificity:** Be precise about what you want.\n",
      "*   **Context:** Provide enough background information for the LLM to understand the task.\n",
      "*   **Examples:** Give examples of the desired output format. (This is often called \"few-shot learning\")\n",
      "*   **Constraints:** Specify any constraints or limitations on the response.\n",
      "*   **Format:** Define the desired output format (e.g., bullet points, code snippets, a specific type of document).\n",
      "*   **Role Play:** Ask the model to act as a particular expert or persona.\n",
      "*   **Iteration and Experimentation:**  Prompt engineering is an iterative process. Experiment with different prompts and analyze the results to refine your approach.\n",
      "\n",
      "**Examples of Prompt Engineering Techniques:**\n",
      "\n",
      "*   **Zero-shot prompting:**  Asking the LLM to perform a task without any examples.\n",
      "    *   Example: \"Translate the following sentence into French: 'Hello, how are you?'\"\n",
      "*   **Few-shot prompting:**  Providing a few examples to guide the LLM's response.\n",
      "    *   Example:\n",
      "        ```\n",
      "        English: I love pizza.\n",
      "        French: J'adore la pizza.\n",
      "\n",
      "        English: I like to dance.\n",
      "        French: J'aime danser.\n",
      "\n",
      "        English: I want to travel.\n",
      "        French:\n",
      "        ```\n",
      "*   **Chain-of-thought prompting:** Guiding the LLM to break down a complex problem into smaller steps.\n",
      "    *   Example: \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? Let's think step by step.\"\n",
      "*   **Role Prompting:** Asking the model to behave as an expert or character. \"You are a helpful AI assistant. Summarize the following article...\"\n",
      "\n",
      "In essence, prompt engineering is a crucial skill for anyone working with LLMs. By mastering the art of prompt creation, you can harness the power of these models to solve a wide range of problems and create innovative applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Q: What is prompt engineering?\n",
    "A: \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un prompt normalmente tiene los siguientes formatos:\n",
    "```sh\n",
    "<Question>?\n",
    "```\n",
    "Or\n",
    "```sh\n",
    "<Instruction>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe aclarar que no es requerido usar este formato ni ningun otro. Siempre podemos adaptar el prompt de acuerdo a nuestras necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "This is awesome! // Positive\n",
    "This is bad! // Negative\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementos de un prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instruccion: La tarea que queremos que haga el modelo.\n",
    "* Contexto: Informacion adicional que puede ayudar al modelo a generar una mejor respuesta.\n",
    "* Input: la pregunta o texto de entrada que queremos que el modelo responda.\n",
    "* Indicador de salida: el tipo, o formato que queremos que el modelo use en su respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Classify the text into neutral, negative, or positive.\n",
    "Text: I think the food was okay.\n",
    "Sentiment:\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consejos Basicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empieza sencillo:\n",
    "* El prompting es un proceso iterativo y requiere mucha prueba y error para llegar a los resultados esperados. Siempre empieza en un entorno de pruebas sencillo.\n",
    "* Empieza siempre con un prompt simple, y ve agregando cosas para que tu prompt vaya produciendo mejores resultados.\n",
    "* Si tienes una tarea grande y compleja, trata de dividirla en tareas mas pequennas y sencillas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La instruccion\n",
    "* Usa palabras simples que instruyan al modelo sobre lo que queremos lograr: *Write*, *Classify*, *Summarize*, *Translate*, *Order*.\n",
    "* De nuevo el prompting es un proceso iterativo y necesitas experimentar para ver que combinacion de intrucciones, palabras clave y contextos te da los mejores resultados para tu tarea.\n",
    "* Otra recomendacion es usar separadores y tags muy bien diferenciados.\n",
    "\n",
    "  Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola!\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_content(\"\"\"\n",
    "### Instruction ###\n",
    "Translate the text below to spanish\n",
    "### Task ###\n",
    "text: \"hello!\"\n",
    "\"\"\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ser especifico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se debe ser muy especifico acerca de la tarea que se quiere realizar. Entre mas descriptivo y detallado sea el prompt, mejores seran los resultados.\n",
    "* Ten en cuenta que aunque hoy dia los modelos tienen tamannos de contextos muy grandes, annadir demasiada informacion al prompt no siempre ayuda. Debes mantener un buen equilibrio entre la cantidad de informacion que le mandas al modelo, y la calidad de esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place: Lisbon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Extract the name of places in the following text. \n",
    "Desired format:\n",
    "Place: <comma_separated_list_of_places>\n",
    "Input: \"Although these developments are encouraging to researchers, much is still a mystery.\n",
    "“We often have a black box between the brain and the effect we see in the periphery,”\n",
    "says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown\n",
    "in Lisbon. “If we want to use it in the therapeutic context, we actually need to understand\n",
    "the mechanism.“\"\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evitar imprecisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recuerda, es muy importante ser detallado y especifico, existe el riesgo de querer pasarse de listo, y esto puede llevar a prompts imprecisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting effective instructions for AI models to elicit desired responses. It involves carefully designing and refining prompts to guide the model towards generating specific, accurate, and useful outputs. Think of it as strategically communicating with AI to get the best results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Explain the concept prompt engineering. Keep the explanation short, only a few sentences, and don't be too descriptive.\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haz mejor esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is like crafting the perfect question to get the best answer from a really smart computer program, like a chatbot. By carefully choosing the words and structure of your request, you can guide the AI to give you more accurate, creative, or useful results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Use 2-3 sentences to explain the concept of prompt engineering to a high school student.\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer o no hacer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un error común que todos cometemos cuando hacemos prompts, es decir que no queremos. En lugar de esto, debemos enfocarnos en lo que queremos lograr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! Based on what's popular and generally well-received right now, I'd recommend:\n",
      "\n",
      "1.  **\"Oppenheimer\"**: A historical drama about the development of the atomic bomb.\n",
      "2.  **\"Barbie\"**: A comedy fantasy film about Barbie and Ken navigating the real world.\n",
      "3.  **\"Spider-Man: Into the Spider-Verse\"**: An animated superhero film with a unique visual style.\n",
      "4.  **\"Everything Everywhere All at Once\"**: A mind-bending action comedy with a lot of heart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "The following is an agent that recommends movies to a customer.\n",
    "Recommends three or four movies. DO NOT ASK FOR INTERESTS.\n",
    "DO NOT ASK FOR PERSONAL INFORMATION.\n",
    "Customer: Please recommend a movie based on my interests.\n",
    "Agent:\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I am not allowed to ask for your interests. I would recommend \"Oppenheimer\" which is currently trending globally.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "The following is an agent that recommends movies to a customer.\n",
    "The agent is responsible to recommend a movie from the top global trending movies.\n",
    "It should refrain from asking users for their preferences and avoid asking for\n",
    "personal information. If the agent doesn't have a movie to recommend,\n",
    "it should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\n",
    "Customer: Please recommend a movie based on my interests.\n",
    "Agent:\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecnicas de Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "El prompt que usamos no contiene ejemplos o demostraciones, es muy efectivo en tareas sencillas, y normalmente siempre es nuestra primera opcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Classify the text into neutral, negative or positive. Answer with a word.\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment: \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "Few-shot learning es una de las tecnicas basicas para proveer al modelo con in-context learning.\n",
    "Es proveer ejemplos o demostraciones para que el modelo entienda mejor la tarea a la hora de generar una respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The little girl was so excited about the ice cream that she started to farduddle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! // \"\"\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"Positive\n",
    "This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! -- \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's identify the odd numbers in the list: 15, 5, 13, 7, 1\n",
      "\n",
      "Now, let's add them together: 15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "So the answer is 41\n",
      "\n",
      "A: 41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is True.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "Instruction:\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cadena de pensamiento permite al modelo razonar a traves de pasos intermedios antes de proveer una respuesta.\n",
    "Se puede combinar con few-shot prompting para obtener mejores resultados en tareas mas complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y funciona igual de bien con menos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El prompt: **Let's think step by step** => **Piensa paso a paso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"strawberry\" contains two \"r\"s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"How many letters *r* contains the word *strawberry*?\"\"\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"strawberry\" has the following letters: s, t, r, a, w, b, e, r, r, y.\n",
      "The letter \"r\" appears three times in the word \"strawberry\".\n",
      "\n",
      "So the answer is 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_content(\"\"\"\n",
    "How many letters *r* contains the word *strawberry*? Let's think step by step.\"\"\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Hoy dia casi todos los modelos hacen CoT por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Prompting es una técnica avanzada que se centra en la estructura y sintaxis de las tareas en lugar de sus detalles específicos. Su objetivo es crear una forma más abstracta y estructurada de interactuar con modelos de lenguaje, priorizando patrones de información sobre el contenido tradicional.\n",
    "\n",
    "### Caracteristicas\n",
    "* Orientado a la estructura: Prioriza el formato y patrón de problemas por encima del contenido específico.\n",
    "* Enfocado en la sintaxis: Usa la sintaxis como una plantilla para la respuesta esperada.\n",
    "* Ejemplos abstractos: Se pueden usar ejemplos para ilustrar la estructura del problema sin enfocarse en detalles en específico.\n",
    "* Versatilidad: Se puede aplicar a una gran variedad de problemas y dominios.\n",
    "* Enfoque Catálogo: Se basa en un método que ayuda a organizar y clasificar la información de manera clara y lógica dentro de un prompt, asegurando que cada parte tenga un propósito definido y facilite una mejor respuesta de la inteligencia artificial.\n",
    "\n",
    "### Ventajas\n",
    "* Eficiencia en tokens: Reduce el número de tokens porque se enfoca en la estructura que definamos en lugar de un contenido detallado.\n",
    "* Comparación justa: La evaluación se puede enfocar en cómo resolvió el problema, un modelo, en lugar de la forma de la respuesta.\n",
    "* Eficacia del Zero-shot: Es como un zero-shot donde la influencia del ejemplo en específico es minimizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let’s think step by step.\n",
      "Let 'c' be the cost of a chocolate and 'a' be the cost of a candy.\n",
      "We can set up a system of two equations based on the given information:\n",
      "\n",
      "Equation 1 (Ana): 3c + 2a = 8.50\n",
      "Equation 2 (Carlos): 2c + 3a = 7.50\n",
      "\n",
      "We can solve this system of equations using either substitution or elimination. Let's use elimination.\n",
      "Multiply Equation 1 by 2: 6c + 4a = 17\n",
      "Multiply Equation 2 by 3: 6c + 9a = 22.50\n",
      "\n",
      "Now, subtract the modified Equation 1 from the modified Equation 2:\n",
      "(6c + 9a) - (6c + 4a) = 22.50 - 17\n",
      "5a = 5.50\n",
      "a = 5.50 / 5\n",
      "a = 1.10\n",
      "\n",
      "Now that we know the cost of a candy (a = 1.10), we can substitute this value back into either Equation 1 or Equation 2 to find the cost of a chocolate. Let's use Equation 1:\n",
      "3c + 2(1.10) = 8.50\n",
      "3c + 2.20 = 8.50\n",
      "3c = 8.50 - 2.20\n",
      "3c = 6.30\n",
      "c = 6.30 / 3\n",
      "c = 2.10\n",
      "\n",
      "So, a chocolate costs $2.10 and a candy costs $1.10.\n",
      "\n",
      "$\\boxed{Chocolate: \\$2.10, Candy: \\$1.10}$\n",
      "\n",
      "The answer is Chocolate: $2.10, Candy: $1.10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_response = model.generate_content(\"\"\"\n",
    "Problem Statement:\n",
    "\n",
    "Problem:\n",
    "Ana and Carlos went to a candy store. Ana bought 3 chocolates and 2 candies for a total of $8.50. Carlos bought 2 chocolates and 3 candies for a total of $7.50.\n",
    "Question: How much does each chocolate and each candy cost?\n",
    "\n",
    "Solution Structure:\n",
    "\n",
    "Begin the response with \"Let’s think step by step.\"\n",
    "Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.\n",
    "End the solution with the final answer encapsulated in a LaTeX-formatted box, [...], for clarity and emphasis.\n",
    "Finally, state \"The answer is [final answer to the problem].\", with the final answer presented in LaTeX notation.\n",
    "-------------\n",
    "\"\"\").text\n",
    "\n",
    "print(latex_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Resolución de la Ecuación Cuadrática\n",
       "\n",
       "Let’s think step by step. Let 'c' be the cost of a chocolate and 'a' be the cost of a candy. We can set up a system of two equations based on the given information:  Equation 1 (Ana): 3c + 2a = 8.50 Equation 2 (Carlos): 2c + 3a = 7.50  We can solve this system of equations using either substitution or elimination. Let's use elimination. Multiply Equation 1 by 2: 6c + 4a = 17 Multiply Equation 2 by 3: 6c + 9a = 22.50  Now, subtract the modified Equation 1 from the modified Equation 2: (6c + 9a) - (6c + 4a) = 22.50 - 17 5a = 5.50 a = 5.50 / 5 a = 1.10  Now that we know the cost of a candy (a = 1.10), we can substitute this value back into either Equation 1 or Equation 2 to find the cost of a chocolate. Let's use Equation 1: 3c + 2(1.10) = 8.50 3c + 2.20 = 8.50 3c = 8.50 - 2.20 3c = 6.30 c = 6.30 / 3 c = 2.10  So, a chocolate costs $$2.10 and a candy costs $$1.10.  $$\\boxed{Chocolate: \\$$2.10, Candy: \\$$1.10}$$  The answer is Chocolate: $$2.10, Candy: $$1.10.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input text (puedes reemplazarlo con cualquier entrada similar)\n",
    "raw_text = latex_response\n",
    "\n",
    "def clean_and_convert_to_markdown(text):\n",
    "    # Remover \\n innecesarios y espacios extra\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Convertir ecuaciones inline $...$ en formato Markdown con MathJax\n",
    "    text = re.sub(r'\\$(.*?)\\$', r'\\\\(\\1\\\\)', text)\n",
    "\n",
    "    # Separar ecuaciones en bloques con $$ ... $$ para MathJax en Markdown\n",
    "    text = re.sub(r'\\\\\\((.*?)\\\\\\)', r'$$\\1$$', text)\n",
    "\n",
    "    # Estructurar salida como Markdown\n",
    "    markdown_template = f\"\"\"# Resolución de la Ecuación Cuadrática\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "    return markdown_template\n",
    "\n",
    "# Generar Markdown limpio\n",
    "markdown_output = clean_and_convert_to_markdown(raw_text)\n",
    "\n",
    "# Mostrar en Jupyter Notebook\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(markdown_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Resolución de la Ecuación Cuadrática\n",
       "\n",
       "Let’s think step by step. We are given the quadratic equation $$x^2 + 4x + 2 = 0$$. We can solve this equation using the quadratic formula. The quadratic formula states that for a quadratic equation of the form $$ax^2 + bx + c = 0$$, the solutions for $$x$$ are given by:  $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$  In our case, $$a = 1$$, $$b = 4$$, and $$c = 2$$. Plugging these values into the quadratic formula, we get:  $$x = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(2)}}{2(1)}$$  $$x = \\frac{-4 \\pm \\sqrt{16 - 8}}{2}$$  $$x = \\frac{-4 \\pm \\sqrt{8}}{2}$$  $$x = \\frac{-4 \\pm 2\\sqrt{2}}{2}$$  $$x = -2 \\pm \\sqrt{2}$$  Therefore, the two solutions for $$x$$ are $$x = -2 + \\sqrt{2}$$ and $$x = -2 - \\sqrt{2}$$.  $$\\boxed{x = -2 \\pm \\sqrt{2}}$$  The answer is $$x = -2 \\pm \\sqrt{2}$$.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex_response = model.generate_content(\"\"\"\n",
    "Problem Statement:\n",
    "\n",
    "Problem: x^2 + 4x + 2 = 0\n",
    "\n",
    "Solution Structure:\n",
    "\n",
    "Begin the response with \"Let’s think step by step.\"\n",
    "Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.\n",
    "End the solution with the final answer encapsulated in a LaTeX-formatted box, [...], for clarity and emphasis.\n",
    "Finally, state \"The answer is [final answer to the problem].\", with the final answer presented in LaTeX notation.\n",
    "-------------\n",
    "\"\"\").text\n",
    "\n",
    "markdown_output = clean_and_convert_to_markdown(latex_response)\n",
    "\n",
    "# Mostrar en Jupyter Notebook\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de seguir un único camino de razonamiento, la auto-consistencia genera varias soluciones diferentes al mismo problema. Luego, se comparan estas soluciones y se elige la más consistente entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a step-by-step breakdown to solve this problem:\n",
      "\n",
      "**1. Understand the problem statement clearly.**\n",
      "\n",
      "*   **Key Variables:**\n",
      "    *   Time to empty the full pond by leakage = 6 hours\n",
      "    *   Water filling rate = 10 liters/hour\n",
      "    *   Pond capacity = 60 liters\n",
      "*   **Goal:** Find the leakage rate in liters/hour.\n",
      "\n",
      "**2. Generate multiple independent reasoning paths, at least three.**\n",
      "\n",
      "*   **Method 1: Focusing on the net change in volume**\n",
      "\n",
      "    1.  Let 'x' be the leakage rate in liters per hour.\n",
      "    2.  The pond empties in 6 hours if only leakage is present. So, Leakage rate = Pond capacity / Time to empty = 60 liters / 6 hours = 10 liters/hour.\n",
      "    3.   Let's rephrase the question: Assume that the leakage rate is 'x' liters per hour and a tap is filling the pool at the rate of 10 litres per hour. The pool gets empty in 6 hours when filled to its max capacity of 60 litres. What is 'x'?\n",
      "    4. If the tank drains in 6 hours then it drains 60/6 litres per hour which is 10 litres per hour.\n",
      "    5. The tank gets drained due to the net flow of water = (leakage rate) - (inflow rate) = x - 10. Therefore x-10 = 10.\n",
      "    6.  Solving for x gives us x=20.\n",
      "\n",
      "*   **Method 2: Considering the filled pond**\n",
      "    1.  Let the leakage rate be 'x' liters/hour.\n",
      "    2. In 6 hours, with only the leak, the pond drains 60 liters. So Leakage rate = 60/6 = 10 litres/hour.\n",
      "    3.  If the tap is opened when the pond is full then in 6 hours, an amount equal to capacity of the pond gets leaked, therefore:\n",
      "    4. 6 \\* (Leakage rate - inflow) = 60\n",
      "    5. 6\\* (x-10) = 60, x-10=10, x=20.\n",
      "\n",
      "*   **Method 3: Incorrect method, will help highlight errors**\n",
      "\n",
      "    1.  The inflow adds 10\\*6 litres of water to the tank = 60 litres\n",
      "    2. Total leakage would then be the sum of pond capacity and the additional water = 60+60 = 120\n",
      "    3. Leakage rate would then be = 120/6 = 20\n",
      "    This is incorrect as it assumes that pond must be filled up by the inflow water, while the inflow tap is always running.\n",
      "\n",
      "**3. Compare the solutions obtained from different methods.**\n",
      "\n",
      "*   Method 1 and Method 2 gave the same result of 20 litres/hour.\n",
      "*   Method 3 incorrectly assumes something and thus will give the wrong answer.\n",
      "\n",
      "**4. Select the most consistent answer.**\n",
      "\n",
      "The leakage rate is 20 liters per hour.\n",
      "\n",
      "Finally, the most consistent answer is **20**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"\"\"\n",
    "Un estanque tiene una fuga y pierde agua constantemente. Si se llena por completo,\n",
    "tarda 6 horas en vaciarse debido a la fuga.\n",
    "Al mismo tiempo, un grifo está vertiendo agua en el estanque a un ritmo de 10 litros por hora.\n",
    "Si la capacidad total del estanque es de 60 litros, ¿cuál es la velocidad de la fuga en litros por hora?\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "1. **Understand the problem statement clearly.**  \n",
    "   - Identify the key variables and conditions given in the problem.  \n",
    "   - Determine what needs to be solved or derived.  \n",
    "\n",
    "2. **Generate multiple independent reasoning paths, at least three.**  \n",
    "   - Approach the problem from different perspectives.  \n",
    "   - Consider alternative ways to derive the answer.  \n",
    "   - Ensure that each method follows a logical and structured reasoning process.  \n",
    "\n",
    "3. **Compare the solutions obtained from different methods.**  \n",
    "   - Check if they lead to the same or similar answers.  \n",
    "   - If there are discrepancies, analyze potential errors or assumptions.  \n",
    "\n",
    "4. **Select the most consistent answer.**  \n",
    "   - If multiple methods converge to the same solution, we can be confident in the result.  \n",
    "   - Clearly state the final answer with justification.  \n",
    "\n",
    "Finally, the most consistent answer is **___**.\n",
    "\n",
    "\"\"\").text\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://www.promptingguide.ai/\n",
    "* https://platform.openai.com/docs/guides/prompt-engineering\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
